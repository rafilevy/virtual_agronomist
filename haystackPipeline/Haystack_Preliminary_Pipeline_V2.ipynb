{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Haystack_Preliminary_Pipeline_V2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7W6CReNgcoI"
      },
      "source": [
        "## Initialisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dx3DvFcESED"
      },
      "source": [
        "# Check for running GPU\r\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oawGovRjEkPF"
      },
      "source": [
        "# Run once for initial installs\r\n",
        "#! pip install farm-haystack\r\n",
        "! pip install git+https://github.com/deepset-ai/haystack.git\r\n",
        "! pip install urllib3==1.25.4\r\n",
        "#! pip install torch==1.6.0+cu101 torchvision==0.6.1+cu101 -f https://download.pytorch.org/whl/torch_stable.html\r\n",
        "!pip install transformers[torch]\r\n",
        "!pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-1.7.0+cu101.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwGqL8YjCX_r"
      },
      "source": [
        "# Run this if want to see graphic representation of pipeline \r\n",
        "! apt install libgraphviz-dev graphviz\r\n",
        "! pip install pygraphviz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OINw-zqjFooP"
      },
      "source": [
        "# Start Elasticsearch from source\r\n",
        "! wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.6.2-linux-x86_64.tar.gz -q\r\n",
        "! tar -xzf elasticsearch-7.6.2-linux-x86_64.tar.gz\r\n",
        "! chown -R daemon:daemon elasticsearch-7.6.2\r\n",
        "! sleep 30"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1JXvtG1KFbu",
        "outputId": "5b7e08c8-483a-4116-8656-3659b47a2c18"
      },
      "source": [
        "# Connect to Elasticsearch - Used for Elastic Retriever \r\n",
        "import os\r\n",
        "from subprocess import Popen, PIPE, STDOUT\r\n",
        "es_server = Popen(['elasticsearch-7.6.2/bin/elasticsearch'],\r\n",
        "                   stdout=PIPE, stderr=STDOUT,\r\n",
        "                   preexec_fn=lambda: os.setuid(1)\r\n",
        "                  )\r\n",
        "from haystack.document_store.elasticsearch import ElasticsearchDocumentStore\r\n",
        "document_store_ES = ElasticsearchDocumentStore(similarity=\"dot_product\", host=\"localhost\", username=\"\", password=\"\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "02/26/2021 21:43:56 - INFO - faiss.loader -   Loading faiss with AVX2 support.\n",
            "02/26/2021 21:43:56 - INFO - faiss.loader -   Loading faiss.\n",
            "02/26/2021 21:43:58 - INFO - farm.modeling.prediction_head -   Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n",
            "02/26/2021 21:43:58 - INFO - elasticsearch -   HEAD http://localhost:9200/ [status:200 request:0.006s]\n",
            "02/26/2021 21:43:58 - INFO - elasticsearch -   HEAD http://localhost:9200/document [status:200 request:0.007s]\n",
            "02/26/2021 21:43:58 - INFO - elasticsearch -   GET http://localhost:9200/document [status:200 request:0.004s]\n",
            "02/26/2021 21:43:58 - INFO - elasticsearch -   PUT http://localhost:9200/document/_mapping [status:200 request:0.014s]\n",
            "02/26/2021 21:43:58 - INFO - elasticsearch -   HEAD http://localhost:9200/label [status:200 request:0.004s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y845gghcCLeh"
      },
      "source": [
        "from typing import List\r\n",
        "import requests\r\n",
        "import pandas as pd\r\n",
        "from haystack import Document\r\n",
        "from haystack.document_store.faiss import FAISSDocumentStore\r\n",
        "\r\n",
        "# FAISS DocumentStore used for DPR and embedding retriever\r\n",
        "document_store_FAISS = FAISSDocumentStore(\r\n",
        "    faiss_index_factory_str=\"Flat\",\r\n",
        "    return_embedding=True\r\n",
        ")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sb566go43uNz"
      },
      "source": [
        "import pprint\r\n",
        "pp = pprint.PrettyPrinter(indent=2)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dW60g6FZgjL0"
      },
      "source": [
        "## Document Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gT3fxP2UKHcS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a895d482-67b0-4d0c-c448-b9c9d60373a2"
      },
      "source": [
        "from haystack.reader.farm import FARMReader\r\n",
        "import haystack\r\n",
        "\r\n",
        "converter = haystack.file_converter.txt.TextConverter(\r\n",
        "                    remove_numeric_tables=False,\r\n",
        "                    valid_languages = [\"en\"])\r\n",
        "\r\n",
        "as4 = converter.convert(file_path=\"/content/as4-winterBarley.txt\")\r\n",
        "\r\n",
        "\r\n",
        "processor = haystack.preprocessor.preprocessor.PreProcessor(\r\n",
        "    clean_empty_lines=True,\r\n",
        "    clean_whitespace=True,\r\n",
        "    clean_header_footer=True,\r\n",
        "    split_by=\"passage\",\r\n",
        "    split_length=1,\r\n",
        "    split_respect_sentence_boundary=False,\r\n",
        "    split_overlap=0\r\n",
        ")\r\n",
        "\r\n",
        "as4Docs = processor.process(as4)\r\n",
        "\r\n",
        "for i in range(len(as4Docs)):\r\n",
        "    as4Docs[i][\"meta\"][\"table\"] = False\r\n",
        "\r\n",
        "document_store_ES.delete_all_documents()\r\n",
        "document_store_ES.write_documents(as4Docs)\r\n",
        "document_store_FAISS.delete_all_documents()\r\n",
        "document_store_FAISS.write_documents(as4Docs)\r\n",
        "\r\n",
        "#backagain = document_store.get_all_documents();\r\n",
        "\r\n",
        "for i in range(0,len(as4Docs)):\r\n",
        "    print(str(i) + \":\", end = \" \")\r\n",
        "    print(as4Docs[i])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "02/26/2021 21:44:05 - INFO - elasticsearch -   POST http://localhost:9200/document/_delete_by_query [status:200 request:0.049s]\n",
            "02/26/2021 21:44:07 - INFO - elasticsearch -   POST http://localhost:9200/_bulk?refresh=wait_for [status:200 request:0.873s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0: {'text': 'Three-spray programmes are recommended for winter barley due to its response to T3 fungicides and the rise in severity of late season ramularia. As such T1 and T2 can be applied slightly earlier if required to better match up with PGR timings.', 'meta': {'_split_id': 0, 'table': False}}\n",
            "1: {'text': 'The main fungicide used for winter barley disease control is Prothioconazole; however growers should look to protect this active by including alternative modes of action.', 'meta': {'_split_id': 1, 'table': False}}\n",
            "2: {'text': 'For winter barley, timing for T1 (GS30-31) is important due to more GAI on lower than on upper leaves. The slightly earlier timing will allow better pairing with an early growth regulator and reduce the need for a T0 treatment.', 'meta': {'_split_id': 2, 'table': False}}\n",
            "3: {'text': 'For winter barley, T1 (GS30-31) application options are:\\nSiltra Xpro 0.4 -0.6 l/ha\\nCebara 1.0 -1.5 l/ha + Proline 0.25 l/ha,\\nComet 0.4 l/ha + Proline 0.3-0.4 l/ha\\nFandango 0.75 -1.0 l/ha\\nElatus Era 0.6 -0.8 l/ha\\nCyprodinil (in Cebara) will have useful activity against eyespot and mildew, also net blotch in which resistance to azoles and SDHIs is increasing.', 'meta': {'_split_id': 3, 'table': False}}\n",
            "4: {'text': 'For winter barley, an earlier timing for the T2 (GS37-39) will make inclusion of ethephon-based PGRs easier.', 'meta': {'_split_id': 4, 'table': False}}\n",
            "5: {'text': 'For winter barley, the justification for high doses at T2 (GS37-39) is difficult to prove as cost effective, the inclusion of a third spray further decreases the need for higher doses of product, in NIAB TAG trials; however brown rust susceptibility and early T2 applications may require higher doses.', 'meta': {'_split_id': 5, 'table': False}}\n",
            "6: {'text': 'For winter barley, T2 (GS43-39) application options are:\\nSiltra Xpro 0.4 -0.6 l/ha\\nCebara + Proline 0.75 -1.0 l/ha + 0.33 l/ha\\nComet + Proline 0.3 l/ha + 0.3 -0.4 l/ha\\nFandango 0.75 -1.0 l/ha\\nPriaxor 0.75-1.0 l/ha\\nElatus Era 0.4 -0.6 l/ha\\nIn all cases add chlorothalonil (CTL) 500 g/ha.', 'meta': {'_split_id': 6, 'table': False}}\n",
            "7: {'text': 'For winter barley, only minimal input is needed for T3 (GS49-59) applications (to top up earlier treatments and provide a vehicle for a second CTL application). Application options are:\\nProline 0.25 l/ha\\nBumper 0.25 l/ha\\nFandango 0.5 l/ha (Only two strobilurin applications permitted per crop)\\nPriaxor 0.75 l/ha (Only two strobilurin applications permitted per crop)\\nIn all cases add chlorothalonil (500 g/ha).', 'meta': {'_split_id': 7, 'table': False}}\n",
            "8: {'text': 'The only reliable control option for ramularia in winter barley is chlorothalonil (CTL).', 'meta': {'_split_id': 8, 'table': False}}\n",
            "9: {'text': 'There has been considerable debate as to whether winter barley needs treating for ramularia at T1: it is unlikely to be controllable at this early timing but in a traditional two-spray programme, with the second treatment applied at GS49 (first awns), there is likely to be a benefit to treating earlier than this.', 'meta': {'_split_id': 9, 'table': False}}\n",
            "10: {'text': 'A three-spray programme for winter barley allows chlorothalonil (CTL) to be used with the latter two sprays, (so no requirement at T1) but if employing a two-spray programme (or three-sprays at T0, T1 and T2) then it would be wise to include chlorothalonil (CTL) at T1.', 'meta': {'_split_id': 10, 'table': False}}\n",
            "11: {'text': 'For winter barley, in high disease pressure seasons (mild wet early spring) T0 sprays have been necessary. Although earlier timings in a T1/T2/T3 programme should remove the need for separate T0 treatment.', 'meta': {'_split_id': 11, 'table': False}}\n",
            "12: {'text': 'Although a three-spray programme is suggested for winter barley, a traditional two-spray approach will still give effective disease control but please note: T0 fungicides are more likely to be needed chlorothalonil (CTL) should be included at T1 if the second treatment is not applied until GS49. In all cases add a morpholine to the T0 if mildew is actively developing or if rusts are present requiring rapid knockdown (e.g. Corbel 0.3 l/ha).', 'meta': {'_split_id': 12, 'table': False}}\n",
            "13: {'text': 'T3: 2018 was a very low disease year but still large responses to fungicide use. Responses to T3 treatment have been higher in the north for some time but responses are still high generally.', 'meta': {'_split_id': 13, 'table': False}}\n",
            "14: {'text': 'Eradicant fungicide activity continues to slip as more resistant Septoria isolates develop, so our control programmes are focussed on preventative disease control. Protectant activity remains good but requires well-timed applications. We also need careful fungicide stewardship to try to prevent further selection of more resistant isolates. Fortunately there has also been a shift to wheat varieties with better disease resistance, which will help reduce disease pressure and slow the development of resistance.', 'meta': {'_split_id': 14, 'table': False}}\n",
            "15: {'text': 'Every year has different levels of disease pressure, often with different diseases being prominent. In 2014/15 there was moderate disease pressure from both Septoria tritici and yellow rust, whereas 2015/16 had very variable septoria and yellow rust levels across the country. 2016/17 had lower Septoria pressure due to a dry winter and very dry spring but yellow rust pressure was higher as was that from brown rust later in the season. 2017/18 had mainly low disease pressure in a season with a long, open autumn; a cold, wet winter; a short late, wet spring and a long, dry, very hot summer.', 'meta': {'_split_id': 15, 'table': False}}\n",
            "16: {'text': 'Crops have been sown over a wide range of sowing dates this autumn and this has resulted in differing levels of disease in the crops at this stage. In the September sowings it is easy to find Septoria, yellow and brown rust and mildew present and active. Later sowing dates should result in less disease pressure but the mild and wet December will allow some disease build up in these crops.', 'meta': {'_split_id': 16, 'table': False}}\n",
            "17: {'text': 'The factors considered in the disease pressure scores are: variety, sowing date, geographical location, the weather, sprayer capacity and attitude to risk.', 'meta': {'_split_id': 17, 'table': False}}\n",
            "18: {'text': 'Later sowing of winter wheat has a profound influence on disease pressure.', 'meta': {'_split_id': 18, 'table': False}}\n",
            "19: {'text': 'Geographic location has an effect on disease pressure for winter wheat, for example lower input septoria treatments would be more appropriate for the east compared to the west/south west under almost any circumstances.', 'meta': {'_split_id': 19, 'table': False}}\n",
            "20: {'text': 'Generally it is the weather condition after a spray that dictates the level of subsequent input required, but warm wet weather prior to treatment will build disease pressure.', 'meta': {'_split_id': 20, 'table': False}}\n",
            "21: {'text': 'Sprayer capacity has an effect on disease pressure as it indicates the ability to spray promptly and minimise the chance of an interruption due to weather. If a farm has the capacity to spray all their wheat crops in a shorter period then it may not need as robust a programme as one that requires several days to cover all the wheat area.', 'meta': {'_split_id': 21, 'table': False}}\n",
            "22: {'text': 'Growers’ attitude to risk is important in assessing disease pressure. However, our recent studies on the economics of wheat disease control have shown that there is a three times greater penalty to margin for spending too little when disease pressure turns out to be high than the same overall spend when disease pressure is lower; so members may wish to be cautious.', 'meta': {'_split_id': 22, 'table': False}}\n",
            "23: {'text': 'The earlier a crop is sown the greater the disease infection it will be exposed to and the larger the amount of inoculum plants will be carrying into the spring. Autumn 2018 did allow later sowings but many farms started earlier and there is a visible difference in the level of disease in crops sown at different times with the later crops carrying much less disease.', 'meta': {'_split_id': 23, 'table': False}}\n",
            "24: {'text': 'There is a three times greater penalty to margin for spending too little when disease pressure turns out to be high than the same overall spend when disease pressure is lower; so members may wish to be cautious.', 'meta': {'_split_id': 24, 'table': False}}\n",
            "25: {'text': 'Here is an example treatment program for winter wheat where there is very high septoria pressure (for example a wet year in the South West). The target cost is £100-£115/ha\\nT0 (Leaf 4) – CTL 500 g/ha (add azole e.g. tebuconazole 50% dose if active rust present, add mildewicide if required\\nT1 (Leaf 3) – CTL 500 g/ha + Aviator 1.0 l/ha or Adexar 1.0 l/ha or Keystone 0.8 l/ha\\nT1.5 (Leaf 2) – CTL 500 g/ha\\nT2 (Flag Leaf) – CTL 500 g/ha + Ascra 1.25 l/ha or Adexar 1.25 l/ha or Elatus Era 1.0 l/ha\\nT3 (Ear) – Proline 0.4 l/ha + tebuconazole 30% dose', 'meta': {'_split_id': 25, 'table': False}}\n",
            "26: {'text': 'Here is an example treatment program for winter wheat where there is very high yellow rust pressure (for example a mild year with a susceptible variety in the East). The target cost is £100-£105/ha\\nT0 (Leaf 4) – CTL 500g/ha + azole e.g. tebuconazole 50% dose if active rust present, or add strobilurin if no active rust, add mildewicide if required\\nT1 (Leaf 3) – CTL 500g/ha + Adexar 1 l/ha or Keystone 0.8 l/ha\\nT1.5 (Leaf 2) – Azole or Strobilurin ± CTL\\nT2 (Flag Leaf) – CTL 500g/ha + Adexar 1.25 l/ha or Elatus Era 1.0 l/ha\\nT3 (Ear) – tebuconazole 75% dose', 'meta': {'_split_id': 26, 'table': False}}\n",
            "27: {'text': 'Here is an example treatment program for winter wheat where there is moderate septoria pressure (for example somewhere in the East). The target cost is £85-£100/ha\\nT0 (Leaf 4) – CTL 500 g/ha (add azole e.g. tebuconazole 50% dose if active rust present, add mildewicide if required\\nT1 (Leaf 3) – CTL 500 g/ha + Aviator 1.0 l/ha or Adexar 1.0 l/ha or Keystone 0.8 l/ha\\nT2 (Flag Leaf) – CTL 500 g/ha + Ascra 1.0 l/ha or Adexar 1.0 l/ha or Elatus Era 0.8 l/ha\\nT3 (Ear) – Proline 275 0.4 l/ha + tebuconazole 30% dose', 'meta': {'_split_id': 27, 'table': False}}\n",
            "28: {'text': 'Here is an example treatment program for winter wheat where there is low septoria pressure (for example a resistant variety in the North). The target cost is £60-£80/ha\\nNo T0\\nT1 (Leaf 3) – CTL 500 g/ha + Proline 275 0.4 l/ha\\nT2 (Flag Leaf) – CTL 500 g/ha + Aviator 1.0 l/ha or Adexar 1.0 l/ha or Keystone 0.8 l/ha\\nT3 (Ear) – tebuconazole 75% dose', 'meta': {'_split_id': 28, 'table': False}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyRO7mk9pu7-",
        "outputId": "253b9b13-9947-479e-ef9b-14e0ea199eb5"
      },
      "source": [
        "# Update table content and table description - csv for table content, txt for table description\r\n",
        "# The files are under the /tables directory, the name does not matter, as long as the csv and the txt match each other.\r\n",
        "# Need to rerun this section when new table is uploaded.\r\n",
        "\r\n",
        "import csv\r\n",
        "import os\r\n",
        "\r\n",
        "data = []\r\n",
        "docs= []\r\n",
        "\r\n",
        "for file in [file for file in os.listdir(os.getcwd()+\"/tables\") if \".csv\" in file]:\r\n",
        "  with open(os.getcwd()+\"/tables/\"+file, mode='r') as infile:\r\n",
        "    reader = csv.reader(infile)\r\n",
        "    new_dict = {row[0]:row[1:] for row in reader}\r\n",
        "    data.append(new_dict)\r\n",
        "  infile.close()\r\n",
        "  with open(os.getcwd()+\"/tables/\"+file[:-4]+\".txt\", mode='r') as infile:\r\n",
        "    docs.append(infile.read())\r\n",
        "  infile.close()\r\n",
        "\r\n",
        "with open('table_text.txt', 'w') as outfile:\r\n",
        "    for item in docs:\r\n",
        "        outfile.write(\"%s\\n\\n\" % item)\r\n",
        "outfile.close()\r\n",
        "\r\n",
        "# Construct FAISS DocumentStore for table content\r\n",
        "\r\n",
        "tables = converter.convert(file_path=\"/content/table_text.txt\")\r\n",
        "\r\n",
        "tableDocs = processor.process(tables)\r\n",
        "for i in range(len(tableDocs)):\r\n",
        "  tableDocs[i][\"meta\"][\"index\"] = i\r\n",
        "  tableDocs[i][\"meta\"][\"table\"] = True\r\n",
        "\r\n",
        "document_store_ES.write_documents(tableDocs)\r\n",
        "document_store_FAISS.write_documents(tableDocs)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "02/26/2021 21:44:10 - INFO - elasticsearch -   POST http://localhost:9200/_bulk?refresh=wait_for [status:200 request:0.525s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ogJS95CIyng",
        "outputId": "002b3886-da5a-476d-ee17-deeb83d1fd2b"
      },
      "source": [
        "# NLP based keyword extractor\r\n",
        "# New category upload is not handled here\r\n",
        "from KeyInfoExtractor import KeywordExtractor\r\n",
        "key_word_extractor = KeywordExtractor()\r\n",
        "key_word_extractor.read_key_info_file(\"categories.csv\")\r\n",
        "key_word_extractor.check_current_categories(True)\r\n",
        "questions = {}\r\n",
        "for ki in key_word_extractor.key_infos:\r\n",
        "  questions[ki.name] = ki.question\r\n",
        "pp.pprint(questions)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Currently held Categories: \n",
            "*  Crop:  ['wheat', 'oats', 'rape', 'peas', 'spring', 'beans', 'barley', 'linseed', 'winter', 'oilseed']  ||  ['', 'spring oilseed rape', 'winter oats', 'winter wheat', 'spring oats', 'winter oilseed rape', 'spring beans', 'spring linseed', 'winter linseed', 'spring wheat', 'spring barley', 'spring peas', 'winter barley', 'winter beans']\n",
            "*  Disease:  ['mildew', 'rhynchosporium', 'brown', 'ear', 'ramularia', 'light', 'botrytis', 'fusarium', 'blight', 'phoma', 'blotch', 'crown', 'eyespot', 'rust', 'canker', 'alternaria', 'chocolate', 'stem', 'leaf', 'downy', 'mycosphaerella', 'septoria', 'verticillium', 'spot', 'sclerotinia', 'net', 'drechslera', 'aschochyta', 'microdochium', 'tan', 'yellow']  ||  ['mildew', 'rhynchosporium', '', 'ramularia', 'botrytis', 'brown rust', 'fusarium', 'light leaf spot', 'yellow rust', 'eyespot', 'drechslera leaf spot', 'phoma leaf spot', 'downy mildew', 'crown rust', 'rust', 'ear blight', 'alternaria', 'mycosphaerella', 'tan spot', 'septoria', 'chocolate spot', 'verticillium', 'sclerotinia', 'net blotch', 'aschochyta', 'phoma stem canker', 'microdochium']\n",
            "*  Fungicide:  ['boogie', '275', 'prothio', 'metal', 'kingdom', 'sentenza', 'nebula', 'rubric', 'chord', '200', 'joules', 'cyflamid', 'coolie', 'phoenix', 'mirador', 'tracker', 'vortex', 'symetra', 'prizm', 'proline', 'corbel', 'chusan', 'syrex', 'filan', 'toledo', 'alto', 'flux', 'index', 'kudos', 'inception', 'amistar', 'toprex', 'comet', 'damocles', 'zulu', 'xtra', 'topsin', 'tardis', 'cielex', 'elate', 'decoy', 'deacon', 'divexo', 'clayton', 'ceratavo', 'fulmar', 'baritone', 'sunorg', 'pro', 'opus', 'siltra', 'odin', 'bumper', 'bonafide', 'highgate', 'cebara', 'banguy', 'fathom', 'smaragdin', 'aylora', 'caramba', 'variano', 'mantra', 'ceriax', 'taurus', 'enterprise', 'era', '90', 'arizona', 'pexan', 'amber', 'bounty', 'spigot', 'tectura', 'pictor', 'corral', 'concorde', 'cortez', 'gringo', 'serpent', 'bontima', 'coli', 'skyway', '500', '250', 'zoro', 'xl', 'propulse', 'librax', 'plus', 'standon', '235', 'beamer', '285', 'supreme', 'xpro', 'morex', 'folicur', 'vertisan', 'crebol', 'priaxor', 'refinzar', 'ascra', 'trust', 'micaraz', 'bowman', 'bravo', 'saltri', 'elatus', 'rover', 'juventus', 'matsuri', 'insignis', 'adexar', 'octolan', 'caryx', 'keystone', 'mitre', 'erase', 'wolverine', 'sparticus', 'apex', 'elite', 'talius', 'aviator', 'epic', 'piper', 'seguris', 'ctl', '60', 'cougar', 'anode', 'imtrex', 'asterix', 'signum', 'treoris', 'bugle', 'whistle', 'intellis', 'orius', 'velogy', 'ignite']  ||  ['proline 275', 'prothio', 'alto elite', 'variano xpro', 'siltra xpro', 'metal', 'kingdom', 'sentenza', 'decoy 250', 'sunorg pro', 'rubric', 'chord', 'intellis plus', 'joules', 'cyflamid', 'phoenix', 'clayton kudos', 'tracker', 'apex pro', 'vortex', 'symetra', 'prizm', 'proline', 'corbel', 'clayton spigot', 'standon beamer', 'chusan', 'metal 60', 'syrex', 'filan', 'toledo', 'rover 500', 'flux', 'nebula xl', 'amistar', 'toprex', 'damocles', 'zulu', 'topsin', 'cielex', 'elate', 'deacon', 'divexo', 'fulmar', 'baritone', 'elatus era', 'opus', 'odin', 'bumper', 'bonafide', 'highgate', 'cebara', 'banguy', 'fathom', 'smaragdin', 'boogie xpro', 'aylora', 'comet 200', 'caramba', 'mantra', 'clayton index', 'ceriax', 'taurus', 'enterprise', 'pexan', 'arizona', 'amber', 'bounty', 'tectura', 'pictor', 'corral', 'concorde', 'cortez', 'gringo', 'serpent', 'bontima', 'ctl 500', 'velogy plus', 'coli', 'skyway', 'aviator 235 xpro', 'clayton tardis', 'zoro', 'ceratavo plus', 'elatus plus', 'librax', 'propulse', 'caramba 90', 'bravo 500', 'skyway 285 xpro', 'supreme', 'standon coolie', 'morex', 'folicur', 'vertisan', 'crebol', 'priaxor', 'refinzar', 'ascra', 'trust', 'sparticus xpro', 'micaraz', 'bowman', 'bravo', 'saltri', 'rover', 'juventus', 'matsuri', 'insignis', 'adexar', 'octolan', 'caryx', 'inception xpro', 'keystone', 'ascra xpro', 'mitre', 'erase', 'wolverine', 'talius', 'epic', 'piper', 'seguris', 'cougar', 'anode', 'imtrex', 'asterix', 'signum', 'treoris', 'bugle', 'whistle', 'intellis', 'orius', 'mirador xtra', 'ignite']\n",
            "*  Timing:  ['ear', 't1', 'second', '4', '2', 't2', '3', 't3', 'mid', 'pod', 'stem', 'early', 'leaf', 'flower', 'flag', 'pre', 'late', 't4', 'extension', 't0', 'set']  ||  ['', 'ear', 'stem extension', 't1', 'mid late flower', 'leaf 3', 'early flower', 'leaf 2', 'leaf 4', 't2', 'mid flower', 't3', 'second ear', 'late pod set', 'early mid flower', 'flag leaf', 'pod set', 't4', 'pre pod set', 't0']\n",
            "*  Area:  ['southwest', 'north', 'southeast', 'south', 'east', 'west']  ||  ['', 'southwest', 'north', 'southeast', 'south', 'east', 'west']\n",
            "\n",
            "{ 'Area': 'Which area are you in? (E.g. east/north/etc)',\n",
            "  'Crop': 'Is there a specific crop that you would like to ask about?',\n",
            "  'Disease': 'Is there a specific disease that you would like to ask about?',\n",
            "  'Fungicide': 'Do you want to check for a specific fungicide?',\n",
            "  'Timing': 'Is there a specific timing that you would like to ask about? '\n",
            "            '(E.g. T0/T1/etc)'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYHZRJHfqai3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d46bae8-6dcc-4f8d-ba2c-a36f04b26eb9"
      },
      "source": [
        "# Preliminary parser for questions - specially tackles the variety of the crops\r\n",
        "# New json upload is not handled here\r\n",
        "from parsing import Parser\r\n",
        "\r\n",
        "parser = Parser()\r\n",
        "\r\n",
        "# To parse question into general case: e.g. what to apply to Dunston -> what to apply to winter wheat\r\n",
        "def question_parsing(text):\r\n",
        "  parsed = parser.parse(text)\r\n",
        "  parsed_string = \"\"\r\n",
        "  for string in parsed[\"simple\"]:\r\n",
        "    parsed_string = parsed_string + string\r\n",
        "  parsed_string = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", parsed_string)\r\n",
        "  parsed_string = re.sub(\"  \", \" \", parsed_string)\r\n",
        "  return parsed_string\r\n",
        "\r\n",
        "# To extract the keywords that the extractor cannot handle - e.g. variety\r\n",
        "def generate_keywords(text):\r\n",
        "  parsed = parser.parse(text)\r\n",
        "  parsed_string = \"\"\r\n",
        "  for string in parsed[\"simple\"]:\r\n",
        "    parsed_string = parsed_string + string\r\n",
        "  keywords = {}\r\n",
        "  while (parsed_string.find(\"(\") != -1):\r\n",
        "    start = parsed_string.find(\"(\")\r\n",
        "    end = parsed_string.find(\")\")\r\n",
        "    bracket = parsed_string[start+1:end]\r\n",
        "    if bracket.find(\":\") != -1:\r\n",
        "      colon = bracket.find(\":\")\r\n",
        "      keywords[bracket[1:colon].replace(\" \", \"\")] = bracket[colon+1:].replace(\" \", \"\")\r\n",
        "    parsed_string = parsed_string[end+1:] \r\n",
        "  return keywords"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpl4rPhd2Wy-"
      },
      "source": [
        "# Pressure score system when crops with different pressure score need to be treated differently.\r\n",
        "# Need to rerun update pressure table if new pressure table is uploaded (need to include the origianl csv content)\r\n",
        "\r\n",
        "import csv\r\n",
        "import re\r\n",
        "\r\n",
        "class PressureScoreGenerator:\r\n",
        "  def __init__(self):\r\n",
        "    self.pressure_table = {}\r\n",
        "\r\n",
        "  def update_pressure_table(self,filename='pressure_score.csv'):\r\n",
        "    with open('pressure_score.csv', mode='r') as infile:\r\n",
        "        reader = csv.reader(infile)\r\n",
        "        for row in reader:\r\n",
        "          if row[0] in self.pressure_table:\r\n",
        "            if row[1] in self.pressure_table[row[0]]:\r\n",
        "              self.pressure_table[row[0]][row[1]][len(self.pressure_table[row[0]][row[1]])+1] = [row[2],int(row[3])]\r\n",
        "            else:\r\n",
        "              self.pressure_table[row[0]][row[1]] = {1:[row[2],int(row[3])]}\r\n",
        "          else:\r\n",
        "            self.pressure_table[row[0]]={row[1]:{1:[row[2],int(row[3])]}}\r\n",
        "    infile.close()\r\n",
        "\r\n",
        "  def calculate_pressure_score(self,crop):\r\n",
        "    if crop in self.pressure_table:\r\n",
        "      print(\"To give a better suggestion. We need to calculate the disease pressure of your current case. Please specify your choice by replying with the number accordingly.\")\r\n",
        "      score = 0\r\n",
        "      # Would it be possible to change this into options by click?\r\n",
        "      for key in self.pressure_table[crop].keys():\r\n",
        "        print(\"Please choose the \" + key + \" which best describes your current case: \")\r\n",
        "        for option in self.pressure_table[crop][key]:\r\n",
        "          print(option, \": \",self.pressure_table[crop][key][option][0])\r\n",
        "        choice = input()\r\n",
        "        while not choice.isnumeric() or not (int(choice) >= 1 and int(choice) <= len(self.pressure_table[crop][key])):\r\n",
        "          choice = input(\"Your input is not Valid. Please check and try again.\")\r\n",
        "        score = score + self.pressure_table[crop][key][int(choice)][1]\r\n",
        "      if score <= 12:\r\n",
        "        pressure_level = \"low\"\r\n",
        "      elif score <= 15:\r\n",
        "        pressure_level = \"moderate\"\r\n",
        "      elif score <= 18:\r\n",
        "        pressure_level = \"high\"\r\n",
        "      else:\r\n",
        "        pressure_level = \"very high\"\r\n",
        "      print(\"Your current disease pressure score is \" + str(score) + \".\\nThis means that the disease pressure is \" + pressure_level + \".\")\r\n",
        "      return pressure_level\r\n",
        "    else:\r\n",
        "      return -1\r\n",
        "\r\n",
        "pressure_score_generator = PressureScoreGenerator()\r\n",
        "pressure_score_generator.update_pressure_table()\r\n",
        "#pp.pprint(pressure_score_generator.pressure_table)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrR-AGlxgqob"
      },
      "source": [
        "## Building Individual Components"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFIjvlTwLSoN"
      },
      "source": [
        "# Naive retriver based on tf * idf - Default BM25, can be cunstomised\r\n",
        "from haystack.retriever.sparse import ElasticsearchRetriever\r\n",
        "es_retriever = ElasticsearchRetriever(document_store=document_store_ES)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMFr-plj4B0o"
      },
      "source": [
        "# Alternative retriever - double BERT neural networks for question and doc embedding\r\n",
        "from haystack.retriever.dense import DensePassageRetriever\r\n",
        "dpr_retriever = DensePassageRetriever(\r\n",
        "    document_store=document_store_FAISS,\r\n",
        "    query_embedding_model=\"facebook/dpr-question_encoder-single-nq-base\",\r\n",
        "    passage_embedding_model=\"facebook/dpr-ctx_encoder-single-nq-base\",\r\n",
        "    use_gpu=True,\r\n",
        "    embed_title=True,\r\n",
        ")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVwNkA6TBRp1",
        "outputId": "b51c8da5-74f2-4a16-c6dd-fe5cd7672d1c"
      },
      "source": [
        "document_store_FAISS.update_embeddings(retriever=dpr_retriever) #possible training of dpr model"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "02/26/2021 21:44:57 - INFO - haystack.document_store.faiss -   Updating embeddings for 31 docs...\n",
            "  0%|          | 0/31 [00:00<?, ?it/s]\n",
            "Creating Embeddings:   0%|          | 0/2 [00:00<?, ? Batches/s]\u001b[A\n",
            "Creating Embeddings:  50%|█████     | 1/2 [00:00<00:00,  3.56 Batches/s]\u001b[A\n",
            "Creating Embeddings: 100%|██████████| 2/2 [00:00<00:00,  3.94 Batches/s]\n",
            "10000it [00:00, 18439.36it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OuTEG8eN9UWm",
        "outputId": "8bcc0023-1e55-401e-fb5b-bd225907eff5"
      },
      "source": [
        "# Alternative retriever - single BERT to embed both question and doc, may be better for similar documents (our case)\r\n",
        "from haystack.retriever.dense import EmbeddingRetriever\r\n",
        "embedding_retriever = EmbeddingRetriever(document_store=document_store_FAISS,\r\n",
        "                               embedding_model=\"deepset/sentence_bert\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "02/26/2021 21:44:57 - INFO - haystack.retriever.dense -   Init retriever using embeddings of model deepset/sentence_bert\n",
            "02/26/2021 21:44:57 - INFO - farm.utils -   Using device: CUDA \n",
            "02/26/2021 21:44:57 - INFO - farm.utils -   Number of GPUs: 1\n",
            "02/26/2021 21:44:57 - INFO - farm.utils -   Distributed Training: False\n",
            "02/26/2021 21:44:57 - INFO - farm.utils -   Automatic Mixed Precision: None\n",
            "02/26/2021 21:45:00 - WARNING - farm.utils -   Failed to log params: Changing param values is not allowed. Param with key='prediction_heads' was already logged with value='TextSimilarityHead' for run ID='a1fe11a62a92431a971f12ad6c5013a9'. Attempted logging new value ''.\n",
            "02/26/2021 21:45:02 - WARNING - farm.utils -   Failed to log params: Changing param values is not allowed. Param with key='processor' was already logged with value='TextSimilarityProcessor' for run ID='a1fe11a62a92431a971f12ad6c5013a9'. Attempted logging new value 'InferenceProcessor'.\n",
            "02/26/2021 21:45:02 - WARNING - farm.utils -   ML Logging is turned off. No parameters, metrics or artifacts will be logged to MLFlow.\n",
            "02/26/2021 21:45:02 - INFO - farm.utils -   Using device: CUDA \n",
            "02/26/2021 21:45:02 - INFO - farm.utils -   Number of GPUs: 1\n",
            "02/26/2021 21:45:02 - INFO - farm.utils -   Distributed Training: False\n",
            "02/26/2021 21:45:02 - INFO - farm.utils -   Automatic Mixed Precision: None\n",
            "02/26/2021 21:45:02 - WARNING - haystack.retriever.dense -   You seem to be using a Sentence Transformer with the dot_product function. We recommend using cosine instead. This can be set when initializing the DocumentStore\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sXdUWDpMzsw"
      },
      "source": [
        "# Reader to further scan with Hugging Face models\r\n",
        "# reader = TransformersReader(model_name_or_path=\"distilbert-base-uncased-distilled-squad\", tokenizer=\"distilbert-base-uncased\", use_gpu=-1)\r\n",
        "# reader = FARMReader(model_name_or_path=\"deepset/bert-large-uncased-whole-word-masking-squad2\", use_gpu=True)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCN5zislt5j0"
      },
      "source": [
        "# Decide whether the answer should be retrieved from the tables or the general texts\r\n",
        "class QueryClassifier:\r\n",
        "  outgoing_edges = 2\r\n",
        "\r\n",
        "  def run(self, **kwargs):\r\n",
        "    #print(\"Running Query Classifier\")\r\n",
        "    #print(len(kwargs[\"documents\"]))\r\n",
        "    #print(kwargs[\"documents\"][0].meta)\r\n",
        "    #for doc in kwargs[\"documents\"]:\r\n",
        "    #  print(doc.text)\r\n",
        "    #print(kwargs[\"documents\"][0].meta[\"table\"] == \"0\")\r\n",
        "    #docs = kwargs[\"documents\"]\r\n",
        "    #for doc in docs:\r\n",
        "    #  print(doc.text)\r\n",
        "    #  print(doc.meta)\r\n",
        "    if (kwargs[\"documents\"][0].meta[\"table\"] == \"0\"):\r\n",
        "      #print(\"It is a general question.\")\r\n",
        "      return (kwargs, \"output_1\")\r\n",
        "    else:\r\n",
        "      #print(\"It is a table-related question.\")\r\n",
        "      return (kwargs, \"output_2\")\r\n",
        "\r\n",
        "query_classifier = QueryClassifier()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ursLVOh7-daI"
      },
      "source": [
        "# Customised retriever - Process table content using TAPAS\r\n",
        "from haystack.retriever.dense import BaseRetriever\r\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForTableQuestionAnswering, TableQuestionAnsweringPipeline\r\n",
        "\r\n",
        "class TableRetriever:\r\n",
        "  outgoing_edges = 1\r\n",
        "\r\n",
        "  def __init__(self):\r\n",
        "    self.tokenizer = AutoTokenizer.from_pretrained(\"google/tapas-base-finetuned-wtq\")\r\n",
        "    self.model = AutoModelForTableQuestionAnswering.from_pretrained(\"google/tapas-base-finetuned-wtq\")\r\n",
        "    self.tableQA = TableQuestionAnsweringPipeline(model=self.model,tokenizer=self.tokenizer)\r\n",
        "\r\n",
        "  def run(self,**kwargs):\r\n",
        "    #print(\"Running Table Retriever\")\r\n",
        "    \r\n",
        "    #print(\"Finished\")\r\n",
        "    return ({\"result\":self.tableQA(data[int(kwargs[\"documents\"][0].meta[\"index\"])], kwargs[\"query\"])},\"output_1\")\r\n",
        "\r\n",
        "table_retriever = TableRetriever()"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIkR1zR4DU__"
      },
      "source": [
        "# Ask further questions if different context is detected.\r\n",
        "# In the special case of pressure score, if it can be calculated, a different set of questions will be asked to decide the score.\r\n",
        "\r\n",
        "class FurtherQuestionGenerator:\r\n",
        "  outgoing_edges = 1\r\n",
        "\r\n",
        "  def individualFiltersGenerator(self, text):\r\n",
        "    text = text.lower()\r\n",
        "    keywords = key_word_extractor.get_best_matches(text)\r\n",
        "    #print(current_filters)\r\n",
        "    #print(key_word_extractor.get_best_matches(text.lower(), give_list=True))\r\n",
        "    \r\n",
        "    # The extraction of pressure keywords work differently from others\r\n",
        "    if text.find(\"pressure\") != -1:\r\n",
        "      keywords[\"pressure\"] = []\r\n",
        "      if \"low\" in text[text.find(\"pressure\")-20:text.find(\"pressure\")+30]:\r\n",
        "        keywords[\"pressure\"].append(\"low\")\r\n",
        "      if \"moderate\" in text[text.find(\"pressure\")-20:text.find(\"pressure\")+30]:\r\n",
        "        keywords[\"pressure\"].append(\"moderate\")\r\n",
        "      if \"high\" in text[text.find(\"pressure\")-20:text.find(\"pressure\")+30]:\r\n",
        "        keywords[\"pressure\"].append(\"high\")\r\n",
        "      if \"very high\" in text[text.lower().find(\"pressure\")-20:text.lower().find(\"pressure\")+30] or \"extreme\" in text[text.find(\"pressure\")-20:text.find(\"pressure\")+30]:\r\n",
        "        keywords[\"pressure\"].append(\"very high\")\r\n",
        "    \r\n",
        "    # To include special keywords such as variety\r\n",
        "    parsed_keywords = generate_keywords(text)\r\n",
        "    if parsed_keywords != {}:\r\n",
        "      for key,value in parsed_keywords.items():\r\n",
        "        if key not in keywords:\r\n",
        "          keywords[key] = [value]\r\n",
        "        else:\r\n",
        "          keywords[key].append(value)\r\n",
        "    return keywords\r\n",
        "\r\n",
        "  def topDocsFilterGenerator(self, docs):\r\n",
        "    return [self.individualFiltersGenerator(doc.text) for doc in docs]\r\n",
        "\r\n",
        "  def filters_difference(self, filters_list, specified = []):\r\n",
        "    current_filters = {}\r\n",
        "    for filters in filters_list:\r\n",
        "      for category, filters in filters.items():\r\n",
        "        if ((category not in specified) and (category in current_filters) and (filters != current_filters[category])):\r\n",
        "          return category\r\n",
        "        elif ((category not in specified) and (category not in current_filters)):\r\n",
        "          current_filters[category] = filters\r\n",
        "    return None\r\n",
        "\r\n",
        "  def furtherQuestions(self, docs, specified=[], original_filters={}):\r\n",
        "    filters_list = self.topDocsFilterGenerator(docs)\r\n",
        "    #print(len(docs),len(filters_list))\r\n",
        "\r\n",
        "    # Filter the retrieved docs before asking questions, eliminate docs with different keywords but keep those does not mention the keywords.\r\n",
        "    for keyword,new_key in original_filters.items():\r\n",
        "      temp_d = [doc for i,doc in enumerate(docs) if not ((keyword in filters_list[i].keys()) and (new_key[0].lower() not in filters_list[i][keyword]))]\r\n",
        "      temp_f = [filters for i,filters in enumerate(filters_list) if not ((keyword in filters_list[i].keys()) and (new_key[0].lower() not in filters_list[i][keyword]))]\r\n",
        "    docs = temp_d\r\n",
        "    filters_list = temp_f\r\n",
        "    #print(len(docs),len(filters_list))\r\n",
        "\r\n",
        "    # Rank the docs based on closeness to keywords in question\r\n",
        "    match = [0 for doc in docs]\r\n",
        "    for keyword,new_key in original_filters.items():\r\n",
        "      match = [match[i] + 1 if ((keyword in filters_list[i].keys()) and (new_key[0].lower() in filters_list[i][keyword])) else match[i] for i in range(len(filters_list))]\r\n",
        "      match = [match[i] + 1 if ((keyword in filters_list[i].keys()) and (filters_list[i][keyword] == [new_key[0].lower()])) else match[i] for i in range(len(filters_list))]\r\n",
        "    \r\n",
        "    # Use only questions from the top 3 results. Otherwise too many questions asked.\r\n",
        "    docs = [x for _,x in sorted(zip(match,docs), key=lambda pair: pair[0], reverse=True)]\r\n",
        "    filters_list = [x for _,x in sorted(zip(match,filters_list), key=lambda pair: pair[0], reverse=True)]\r\n",
        "    keyword = self.filters_difference(filters_list[:3],specified)\r\n",
        "    contained_pressure = False\r\n",
        "    while keyword is not None:\r\n",
        "      if keyword != \"pressure\":\r\n",
        "        new_key = input(questions[keyword] + \"(If not, please reply No) \")\r\n",
        "        if (new_key.lower() != \"no\"):\r\n",
        "          match = [match[i] + 1 if ((keyword in filters_list[i].keys()) and (new_key.lower() in filters_list[i][keyword])) else match[i] for i in range(len(filters_list))]\r\n",
        "          match = [match[i] + 1 if ((keyword in filters_list[i].keys()) and (filters_list[i][keyword] == [new_key.lower()])) else match[i] for i in range(len(filters_list))]\r\n",
        "        specified.append(keyword)\r\n",
        "        original_filters[keyword] = [new_key]\r\n",
        "      else:\r\n",
        "        contained_pressure = True\r\n",
        "        specified.append(keyword)\r\n",
        "        original_filters[keyword] = []\r\n",
        "      keyword = self.filters_difference(filters_list[:3],specified)\r\n",
        "    \r\n",
        "    # Treat the special set of pressure score questions\r\n",
        "    if contained_pressure and \"Crop\" in original_filters:\r\n",
        "        pressure_level = pressure_score_generator.calculate_pressure_score(original_filters[\"Crop\"][0])\r\n",
        "        match = [match[i] + 1 if ((\"pressure\" in filters_list[i].keys()) and (pressure_level in filters_list[i][\"pressure\"])) else match[i] for i in range(len(filters_list))]\r\n",
        "        match = [match[i] + 1 if ((\"pressure\" in filters_list[i].keys()) and (filters_list[i][\"pressure\"] == [pressure_level])) else match[i] for i in range(len(filters_list))]\r\n",
        "    sorted_docs = sorted(zip(match,docs), key=lambda pair: pair[0], reverse=True)\r\n",
        "    #for doc in sorted_docs:\r\n",
        "      #print(doc[1].text)\r\n",
        "      #print(doc[1].meta)\r\n",
        "    return sorted_docs[:3]\r\n",
        "\r\n",
        "  def run(self, **kwargs):\r\n",
        "    #print(\"Running Question Generator\")\r\n",
        "    original_filters = self.individualFiltersGenerator(kwargs[\"query\"])\r\n",
        "    specified = list(original_filters.keys())\r\n",
        "    return ({\"result\": self.furtherQuestions(kwargs[\"documents\"],specified,original_filters)},\"output_1\")\r\n",
        "\r\n",
        "question_generator = FurtherQuestionGenerator()"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wugdBgtxoaH"
      },
      "source": [
        "class Result:\r\n",
        "  outgoing_edges = 1\r\n",
        "\r\n",
        "  def run(self,**kwargs):\r\n",
        "    #print(\"Running Result\")\r\n",
        "    #print(type(kwargs[\"result\"]))\r\n",
        "    if type(kwargs[\"result\"]) is list:\r\n",
        "      docs = [doc[1] for doc in kwargs[\"result\"]]\r\n",
        "      return (docs,\"output_1\")\r\n",
        "    else:\r\n",
        "      return (kwargs[\"result\"][\"answer\"],\"output_1\")\r\n",
        "\r\n",
        "result = Result()"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHatnDA3g2ON"
      },
      "source": [
        "## Assembling into Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HEH2V1xi3Hw5",
        "outputId": "83999776-c911-48d7-c2ed-07866a28fd86"
      },
      "source": [
        "# Current approach\r\n",
        "from haystack import Pipeline\r\n",
        "from haystack.pipeline import JoinDocuments\r\n",
        "\r\n",
        "# Building new pipeline with multiple retrievers\r\n",
        "pipeline = Pipeline()\r\n",
        "pipeline.add_node(component=es_retriever, name=\"ESRetriever\", inputs=[\"Query\"])\r\n",
        "pipeline.add_node(component=dpr_retriever, name=\"DPRRetriever\", inputs=[\"Query\"])\r\n",
        "pipeline.add_node(component=embedding_retriever, name=\"EmbeddingRetriever\", inputs=[\"Query\"])\r\n",
        "pipeline.add_node(component=JoinDocuments(join_mode=\"merge\"), name=\"JoinResults\", inputs=[\"DPRRetriever\",\"EmbeddingRetriever\",\"ESRetriever\"])\r\n",
        "pipeline.add_node(component=query_classifier, name=\"QueryClassifier\", inputs=[\"JoinResults\"])\r\n",
        "pipeline.add_node(component=question_generator, name=\"QnGenerator\", inputs=[\"QueryClassifier.output_1\"])\r\n",
        "pipeline.add_node(component=table_retriever, name=\"TableRetriever\", inputs=[\"QueryClassifier.output_2\"])\r\n",
        "pipeline.add_node(component=result, name=\"Result\", inputs=[\"QnGenerator\", \"TableRetriever\"])\r\n",
        "pipeline.draw(path=\"custom_pipe.png\")\r\n",
        "\r\n",
        "# Question input goes here\r\n",
        "question = input(\"What would you like to ask about?\")\r\n",
        "response = pipeline.run(query=question_parsing(question), top_k_retriever=20)\r\n",
        "\r\n",
        "# Final answer is here \r\n",
        "#final_result = response.text if type(response) is Document else response[\"answer\"]\r\n",
        "if type(response) is list:\r\n",
        "  print(response[0].text)\r\n",
        "else:\r\n",
        "  print(response)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "What would you like to ask about?What to use for winter wheat?\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 497.90it/s]\n",
            "02/26/2021 22:33:09 - INFO - elasticsearch -   POST http://localhost:9200/document/_search [status:200 request:0.010s]\n",
            "02/26/2021 22:33:09 - WARNING - farm.data_handler.processor -   Currently no support in InferenceProcessor for returning problematic ids\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00, 17.50 Batches/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 513.19it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 517.56it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1793.20it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 2579.52it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 857.20it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 723.41it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 382.73it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 734.10it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 370.91it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 940.22it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 1607.01it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 1657.83it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 744.33it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00, 4266.84it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2073.82it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2074.85it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00, 2624.17it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00, 1152.60it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00, 874.60it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2098.73it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 792.57it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 776.15it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00, 1062.39it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 856.07it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 725.28it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 853.54it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 975.99it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1158.01it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1978.45it/s]\n",
            "100%|██████████| 4/4 [00:00<00:00, 2354.70it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00, 1647.62it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 786.70it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 851.12it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 1112.55it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00, 2402.24it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 908.05it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00, 926.51it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 218.49it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 834.02it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1441.09it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00, 2775.23it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 1217.86it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 462.18it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Which area are you in? (E.g. east/north/etc)(If not, please reply No) East\n",
            "To give a better suggestion. We need to calculate the disease pressure of your current case. Please specify your choice by replying with the number accordingly.\n",
            "Please choose the varieties which best describes your current case: \n",
            "1 :  KWS Extase\n",
            "2 :  Dunston/Graham/KWS Zyatt/Moulton/LG Motown/LG Sundance/RGT Illustrious/KWS Firefly\n",
            "3 :  Bennington/Costello/Elicit/Freiston/Gleam/KWS Crispin/KWS Siskin/Revelation/Skyfall\n",
            "4 :  Crusoe/Evolution/KWS Kerrin/KWS Silverstone/KWS Trinity/LG Skyscraper/LG Spotlight/Shabras\n",
            "5 :  Claire/Dickens/Elation/Grafton/JB Diego/KWS Basset/KWS Jackal/KWS Lili/LG Detroit/RGT Gravity/Savello/Viscount/Zulu\n",
            "6 :  Cordiale/Gallant/KWS Barrel/Leeds/LG Jigsaw/LG Interstellar/LG Rhythm/LG Sabertooth/Myriad/Reflection/KWS Santiago/SY Loki\n",
            "1\n",
            "Please choose the area in the country which best describes your current case: \n",
            "1 :  East\n",
            "2 :  North\n",
            "3 :  South East\n",
            "4 :  West\n",
            "5 :  South\n",
            "6 :  South West\n",
            "2\n",
            "Please choose the sowing date which best describes your current case: \n",
            "1 :  Early September\n",
            "2 :  Mid-September\n",
            "3 :  Late September\n",
            "4 :  Early October\n",
            "5 :  Mid-October\n",
            "6 :  Late October\n",
            "7 :  November onwards\n",
            "2\n",
            "Please choose the weather in  November and December in your area which best describes your current case: \n",
            "1 :  Wet Mild Winter\n",
            "2 :  Dry Cool Winter\n",
            "2\n",
            "Please choose the weather in January to March in your area which best describes your current case: \n",
            "1 :  Wet Mild Early Spring\n",
            "2 :  Dry Mild Early Spring\n",
            "3 :  Dry Cool Early Spring\n",
            "2\n",
            "Please choose the sprayer capacity which best describes your current case: \n",
            "1 :  All Wheat in 1 day\n",
            "2 :  All wheat in 2-3 days\n",
            "3 :  4 days or more\n",
            "2\n",
            "Please choose the attitude to risk which best describes your current case: \n",
            "1 :  cautious\n",
            "2 :  default\n",
            "3 :  take a risk\n",
            "2\n",
            "Your current disease pressure score is 13.\n",
            "This means that the disease pressure is moderate.\n",
            "Here is an example treatment program for winter wheat where there is moderate septoria pressure (for example somewhere in the East). The target cost is £85-£100/ha\n",
            "T0 (Leaf 4) – CTL 500 g/ha (add azole e.g. tebuconazole 50% dose if active rust present, add mildewicide if required\n",
            "T1 (Leaf 3) – CTL 500 g/ha + Aviator 1.0 l/ha or Adexar 1.0 l/ha or Keystone 0.8 l/ha\n",
            "T2 (Flag Leaf) – CTL 500 g/ha + Ascra 1.0 l/ha or Adexar 1.0 l/ha or Elatus Era 0.8 l/ha\n",
            "T3 (Ear) – Proline 275 0.4 l/ha + tebuconazole 30% dose\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}