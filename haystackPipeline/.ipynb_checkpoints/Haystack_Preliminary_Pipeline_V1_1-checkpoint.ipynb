{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_dx3DvFcESED"
   },
   "outputs": [],
   "source": [
    "# Check for running GPU\r\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oawGovRjEkPF"
   },
   "outputs": [],
   "source": [
    "#run once for initial installs\r\n",
    "! pip install farm-haystack\r\n",
    "! pip install git+https://github.com/deepset-ai/haystack.git\r\n",
    "! pip install urllib3==1.25.4\r\n",
    "! pip install torch==1.6.0+cu101 torchvision==0.6.1+cu101 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "OINw-zqjFooP"
   },
   "outputs": [],
   "source": [
    "#Start Elasticsearch from source\r\n",
    "! wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.6.2-linux-x86_64.tar.gz -q\r\n",
    "! tar -xzf elasticsearch-7.6.2-linux-x86_64.tar.gz\r\n",
    "! chown -R daemon:daemon elasticsearch-7.6.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F1JXvtG1KFbu",
    "outputId": "53456f45-7378-480b-c53f-43bbb14b3de1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/09/2021 00:19:43 - INFO - elasticsearch -   PUT http://localhost:9200/document [status:200 request:0.381s]\n",
      "02/09/2021 00:19:43 - INFO - elasticsearch -   PUT http://localhost:9200/label [status:200 request:0.201s]\n"
     ]
    }
   ],
   "source": [
    "# Connect to Elasticsearch\r\n",
    "import os\r\n",
    "from subprocess import Popen, PIPE, STDOUT\r\n",
    "es_server = Popen(['elasticsearch-7.6.2/bin/elasticsearch'],\r\n",
    "                   stdout=PIPE, stderr=STDOUT,\r\n",
    "                   preexec_fn=lambda: os.setuid(1)\r\n",
    "                  )\r\n",
    "from haystack.document_store.elasticsearch import ElasticsearchDocumentStore\r\n",
    "document_store = ElasticsearchDocumentStore(host=\"localhost\", username=\"\", password=\"\", index=\"document\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gT3fxP2UKHcS"
   },
   "outputs": [],
   "source": [
    "from haystack.preprocessor.cleaning import clean_wiki_text\r\n",
    "from haystack.preprocessor.utils import convert_files_to_dicts, fetch_archive_from_http\r\n",
    "from haystack.reader.farm import FARMReader\r\n",
    "\r\n",
    "# Testing with tutorial data\r\n",
    "doc_dir = \"data/article_txt_got\"\r\n",
    "s3_url = \"https://s3.eu-central-1.amazonaws.com/deepset.ai-farm-qa/datasets/documents/wiki_gameofthrones_txt.zip\"\r\n",
    "fetch_archive_from_http(url=s3_url, output_dir=doc_dir)\r\n",
    "dicts = convert_files_to_dicts(dir_path=doc_dir, clean_func=clean_wiki_text, split_paragraphs=True)\r\n",
    "\r\n",
    "# Connecting point to preprocessing of as4\r\n",
    "document_store.write_documents(dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "TFIjvlTwLSoN"
   },
   "outputs": [],
   "source": [
    "# Fast filter to narrow down text - Default BM25, can be cunstomised\r\n",
    "from haystack.retriever.sparse import ElasticsearchRetriever\r\n",
    "retriever = ElasticsearchRetriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0sXdUWDpMzsw"
   },
   "outputs": [],
   "source": [
    "# Reader to further scan with Hugging Face models\r\n",
    "# reader = TransformersReader(model_name_or_path=\"distilbert-base-uncased-distilled-squad\", tokenizer=\"distilbert-base-uncased\", use_gpu=-1)\r\n",
    "reader = FARMReader(model_name_or_path=\"deepset/bert-large-uncased-whole-word-masking-squad2\", use_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "iwIgVaYeNPoS"
   },
   "outputs": [],
   "source": [
    "from haystack.pipeline import ExtractiveQAPipeline\r\n",
    "\r\n",
    "# Original Finder deprecated, pipeline allows more flexibility\r\n",
    "# prediction = finder.get_answers(question=\"Who is the father of Arya Stark?\", top_k_retriever=10, top_k_reader=5)\r\n",
    "\r\n",
    "# top_k_retriever -> the more retriever the more document scanned in Reader, slower but higher hit rate\r\n",
    "extractive_pipeline = ExtractiveQAPipeline(reader=reader, retriever=retriever) # Other options: Document Search, Generative, FAQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s1EDRXomPo4X",
    "outputId": "b39a0175-31d8-44df-985c-2c80e1c93419"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What would you like to ask? the father of arya stark\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/09/2021 01:04:47 - INFO - elasticsearch -   POST http://localhost:9200/document/_search [status:200 request:0.012s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00, 10.68 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.48 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00, 19.09 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00, 18.60 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00, 18.40 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  9.66 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  9.85 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  6.59 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.27 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  9.27 Batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eddard\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "question = input(\"What would you like to ask? \")\r\n",
    "prediction = extractive_pipeline.run(query=question, top_k_retriever=10, top_k_reader=5)\r\n",
    "# print_answers(prediction, details=\"all\") #details: all, medium, minimal\r\n",
    "# data format: {query,'answers':[{'answer','score','probability','context','document_id','offset','meta'}]}\r\n",
    "print(prediction['answers'][0]['answer'])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Haystack_Preliminary_Pipeline_V1.0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
