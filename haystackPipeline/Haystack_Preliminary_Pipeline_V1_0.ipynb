{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Haystack_Preliminary_Pipeline_V1.0.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dx3DvFcESED"
      },
      "source": [
        "# Check for running GPU\r\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oawGovRjEkPF"
      },
      "source": [
        "#run once for initial installs\r\n",
        "! pip install farm-haystack\r\n",
        "! pip install git+https://github.com/deepset-ai/haystack.git\r\n",
        "! pip install urllib3==1.25.4\r\n",
        "! pip install torch==1.6.0+cu101 torchvision==0.6.1+cu101 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OINw-zqjFooP"
      },
      "source": [
        "#Start Elasticsearch from source\r\n",
        "! wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.6.2-linux-x86_64.tar.gz -q\r\n",
        "! tar -xzf elasticsearch-7.6.2-linux-x86_64.tar.gz\r\n",
        "! chown -R daemon:daemon elasticsearch-7.6.2"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1JXvtG1KFbu",
        "outputId": "53456f45-7378-480b-c53f-43bbb14b3de1"
      },
      "source": [
        "# Connect to Elasticsearch\r\n",
        "import os\r\n",
        "from subprocess import Popen, PIPE, STDOUT\r\n",
        "es_server = Popen(['elasticsearch-7.6.2/bin/elasticsearch'],\r\n",
        "                   stdout=PIPE, stderr=STDOUT,\r\n",
        "                   preexec_fn=lambda: os.setuid(1)\r\n",
        "                  )\r\n",
        "from haystack.document_store.elasticsearch import ElasticsearchDocumentStore\r\n",
        "document_store = ElasticsearchDocumentStore(host=\"localhost\", username=\"\", password=\"\", index=\"document\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "02/09/2021 00:19:43 - INFO - elasticsearch -   PUT http://localhost:9200/document [status:200 request:0.381s]\n",
            "02/09/2021 00:19:43 - INFO - elasticsearch -   PUT http://localhost:9200/label [status:200 request:0.201s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gT3fxP2UKHcS"
      },
      "source": [
        "from haystack.preprocessor.cleaning import clean_wiki_text\r\n",
        "from haystack.preprocessor.utils import convert_files_to_dicts, fetch_archive_from_http\r\n",
        "from haystack.reader.farm import FARMReader\r\n",
        "\r\n",
        "# Testing with tutorial data\r\n",
        "doc_dir = \"data/article_txt_got\"\r\n",
        "s3_url = \"https://s3.eu-central-1.amazonaws.com/deepset.ai-farm-qa/datasets/documents/wiki_gameofthrones_txt.zip\"\r\n",
        "fetch_archive_from_http(url=s3_url, output_dir=doc_dir)\r\n",
        "dicts = convert_files_to_dicts(dir_path=doc_dir, clean_func=clean_wiki_text, split_paragraphs=True)\r\n",
        "\r\n",
        "# Connecting point to preprocessing of as4\r\n",
        "document_store.write_documents(dicts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFIjvlTwLSoN"
      },
      "source": [
        "# Fast filter to narrow down text - Default BM25, can be cunstomised\r\n",
        "from haystack.retriever.sparse import ElasticsearchRetriever\r\n",
        "retriever = ElasticsearchRetriever(document_store=document_store)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sXdUWDpMzsw"
      },
      "source": [
        "# Reader to further scan with Hugging Face models\r\n",
        "# reader = TransformersReader(model_name_or_path=\"distilbert-base-uncased-distilled-squad\", tokenizer=\"distilbert-base-uncased\", use_gpu=-1)\r\n",
        "reader = FARMReader(model_name_or_path=\"deepset/bert-large-uncased-whole-word-masking-squad2\", use_gpu=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwIgVaYeNPoS"
      },
      "source": [
        "from haystack.pipeline import ExtractiveQAPipeline\r\n",
        "\r\n",
        "# Original Finder deprecated, pipeline allows more flexibility\r\n",
        "# prediction = finder.get_answers(question=\"Who is the father of Arya Stark?\", top_k_retriever=10, top_k_reader=5)\r\n",
        "\r\n",
        "# top_k_retriever -> the more retriever the more document scanned in Reader, slower but higher hit rate\r\n",
        "extractive_pipeline = ExtractiveQAPipeline(reader=reader, retriever=retriever) # Other options: Document Search, Generative, FAQ"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1EDRXomPo4X",
        "outputId": "b39a0175-31d8-44df-985c-2c80e1c93419"
      },
      "source": [
        "question = input(\"What would you like to ask? \")\r\n",
        "prediction = extractive_pipeline.run(query=question, top_k_retriever=10, top_k_reader=5)\r\n",
        "# print_answers(prediction, details=\"all\") #details: all, medium, minimal\r\n",
        "# data format: {query,'answers':[{'answer','score','probability','context','document_id','offset','meta'}]}\r\n",
        "print(prediction['answers'][0]['answer'])"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "What would you like to ask? the father of arya stark\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/09/2021 01:04:47 - INFO - elasticsearch -   POST http://localhost:9200/document/_search [status:200 request:0.012s]\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00, 10.68 Batches/s]\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.48 Batches/s]\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00, 19.09 Batches/s]\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00, 18.60 Batches/s]\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00, 18.40 Batches/s]\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  9.66 Batches/s]\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  9.85 Batches/s]\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  6.59 Batches/s]\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.27 Batches/s]\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  9.27 Batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Eddard\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}